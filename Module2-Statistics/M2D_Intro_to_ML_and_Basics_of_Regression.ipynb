{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-HqCH6grcQn"
      },
      "source": [
        "<html>\n",
        "    <summary></summary>\n",
        "         <div> <p></p> </div>\n",
        "         <div style=\"font-size: 20px; width: 800px;\"> \n",
        "              <h1>\n",
        "               <left>Intro to Machine Learning for Regression and Classification.</left>\n",
        "              </h1>\n",
        "              <p><left>============================================================================</left> </p>\n",
        "<pre>Course: BIOM 440, Spring 2026\n",
        "Instructor: Brian Munsky\n",
        "Authors: Zachary Fox, William Raymond, Brian Munsky\n",
        "Contact Info: munsky@colostate.edu\n",
        "</pre>\n",
        "         </div>\n",
        "    </p>\n",
        "\n",
        "</html>\n",
        "\n",
        "<details>\n",
        "  <summary>Copyright info</summary>\n",
        "\n",
        "```\n",
        "Copyright 2024 Brian Munsky\n",
        "\n",
        "Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n",
        "\n",
        "1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n",
        "\n",
        "2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n",
        "\n",
        "3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n",
        "\n",
        "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
        "```\n",
        "<details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Package imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # New packages that we are going to use in this notebook.\n",
        "# # Uncomment the following lines to install the packages.\n",
        "#\n",
        "# %pip install numpy==1.25\n",
        "# # This downgrades numpy to version 1.25.0, which is compatible with the current version of torch as of 02/23/2025. If you\n",
        "# # have a newer version of numpy installed, you may encounter errors when using torch. After downgrading numpy, you may need\n",
        "# # to restart the kernel to apply the changes.  You may also need to reinstall other packages that depend on numpy (such as\n",
        "# # scikit-learn).\n",
        "#\n",
        "# # New packages:\n",
        "# %pip install torch\n",
        "# %pip install scikit-learn\n",
        "# %pip install tdqm\n",
        "# # These may take a few minutes to install.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import r2_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFxav8kxlzwe"
      },
      "source": [
        "# **Learning Objectives**\n",
        "\n",
        "Upon completion of this lesson, you should be able to:\n",
        "* Describe the **what** are **Regression**,  **Classification**, and **Machine Learning**. \n",
        "* Describe **when**/**why** to use these approaches.\n",
        "* Describe the theory behind **Linear Regression**\n",
        "* Write your own codes to perform a **Linear Regression**\n",
        "* Correct for **Bias** in a linear regression model\n",
        "* Perform **Regularization** for linear regression\n",
        "* Extend linear regression to **Polynomial Regression**\n",
        "* Find and utlize a modern machine learning tools for regression, including **Neural Networks**, **Random Forests**, and **Bayesian Regression**.\n",
        "\n",
        "Some relevant books (with available PDFs):\n",
        "* [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf), Christopher Bishop\n",
        "\n",
        "* [Probabilistic Machine Learning](https://probml.github.io/pml-book/book1.html), Kevin Patrick Murphy\n",
        "\n",
        "* [Data Modeling for the Sciences: Applications, Basics, Computations](https://www.amazon.com/Data-Modeling-Sciences-Applications-Computations/dp/1009098500/ref=sr_1_1?crid=3ACTTNI3IZ2SA&keywords=Steve+Presse+book&qid=1685627119&sprefix=steve+presse+boo%2Caps%2C317&sr=8-1), Steve Press√© and Ioannis Sgouralis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **1. Machine Learning Overview**\n",
        "\n",
        "In this notebook and lecture, we are going to look at some supervised machine learning basics. Let's start off with a definition:\n",
        "\n",
        "Machine learning (ML) is a class of mathematics and algorithms that learn an arbitrary model (denoted as $\\mathcal{M}$) from sample data (denoted as $\\bar{X}$) in order to make some predicitons or decisions (denoted as $\\bar{Y}$). ML models are usually found by minimizing a given loss function ($L$), which in many cases is related to maximizing some likelihood function. \n",
        "\n",
        "Sample data can be provided with or without explanatory labels, and the model to be learned is not explicitly programmed, hence the name \"Machine Learning.\" Broadly, ML falls under the umbrella of Artificial Intelligence, although that field has other disciplines besides ML. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p align=\"center\">  <img src=\"FiguresD/image.png\"> </p>\n",
        "\n",
        "In biomedical engineering, machine learning techniques are applied to analyze and interpret large datasets of biological and medical information, such as genetic data, medical images, or clinical records, to discover patterns, identify relationships, and assist in diagnosis, prognosis, and treatment planning.\n",
        "\n",
        "There are lots of different types of Machine Learning, each with its own strengths and limitations.  We will only cover a few in this course, but I hope that you will explore more on your own."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUNyYKSUFZGY"
      },
      "source": [
        "## **1.A. What are ML models?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JcLV23ts4cQ"
      },
      "source": [
        "<p align=\"center\">   <img src=\"FiguresD/image1.png\"> </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **1.A.1. The cannonical machine learning equation (X -> M -> Y)**\n",
        "\n",
        "In much of machine learning, we are faced with the challenge to  approximate some relationship:\n",
        "$$\\mathbf{X} \\rightarrow \\mathcal{M} \\rightarrow \\mathbf{Y}, $$\n",
        "but you might wonder, what are these quantities $\\mathbf{X}$, $\\mathcal{M}$, and $\\mathbf{Y}$?  That is a great question, because the real power of machine learning is that all three of these terms can mean a very large number of very different things for different problems, and yet the concepts and approaches to solve this problem can remain similar.  Let's define each in very general terms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p align=\"center\">   <img src=\"FiguresD/images.001.png\"> </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **1.B. \"Input Features\" ($\\mathbf{X}$)**.\n",
        "\n",
        "The matrix $\\bar{X}_i$ is your **input feature data**; this is a collection of data that you can obtain to quantify each particular $i^{\\rm th}$ sample, event or experiment. This can take many shapes, forms and sizes. One of the most common is an unordered vector of some arbitrary features. For example, one could imagine a classifier algorithm that takes inputs about a patient's currenty symptoms and demographics (perhaps we want to predict a probability that this patient has diabetes or another disease). That input data may be formatted as such:\n",
        "\n",
        "$$\n",
        "  \\bar{X} = [ \\text{Int : Age}, \\text{Int : Urinations per day},  \\text{Float : Liters drank per day}, \\text{Bool : Peripheral Numbness}]\n",
        "$$\n",
        "\n",
        "An example entry for the $i^{\\rm th}$ patient may be:\n",
        "$$\\bar{X}_i = [ 27, 6, 1.5, True ]$$\n",
        "\n",
        "When we have data on a large number of different samples, we typically order these features into a matrix with one row for each sample:\n",
        "\n",
        "$$\\mathbf{X} =\\left[ \\begin{matrix} \\bar{X}_0 \\\\ \\bar{X}_1 \\\\ \\vdots \\\\ \\bar{X}_{N-1} \\end{matrix}\\right]$$\n",
        "\n",
        "Ordered datasets can come in many shapes ranging from something like text (1xN vector where sequence is important) to full 3D color movies (XYZ coordinates by Color channel by number of frames). All of these can be represented by an arbitrary tensor of the correct shape by number of entries in your dataset.\n",
        "\n",
        "$$\n",
        "Shape(\\mathbf{X}) = Nsamples \\times Dim_1 \\times Dim_2 ... Dim_N\n",
        "$$\n",
        "**The first step in any machine learning problem is always to look at the data**, so let's do that.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![alt text](FiguresD/images.006.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **1.B.1. Python example of input matrix for diabetes data set.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example of some input feature data for the diabetes dataset.\n",
        "diabetes = datasets.load_diabetes()\n",
        "dataDiabetes = diabetes.data\n",
        "\n",
        "# Note the following features in the diabetes dataset:\n",
        "    # age - age in years\n",
        "    # sex - male or female\n",
        "    # bmi - body mass index\n",
        "    # bp - average blood pressure\n",
        "    # s1 - TC: total serum cholesterol\n",
        "    # s2 - LDL: low-density lipoproteins\n",
        "    # s3 - HDL: high-density lipoproteins\n",
        "    # s4 - TCH: total cholesterol / HDL\n",
        "    # s5 - LTG: possibly log of serum triglycerides level\n",
        "    # s6 - GLU: blood sugar level\n",
        "\n",
        "# Let's update the data labels to be more descriptive.\n",
        "diabetes.feature_names[4] = 'TC' # Total cholesterol\n",
        "diabetes.feature_names[5] = 'LDL' # Low-density lipoproteins\n",
        "diabetes.feature_names[6] = 'HDL' # High-density lipoproteins\n",
        "diabetes.feature_names[7] = 'TCH' # Total cholesterol / HDL\n",
        "diabetes.feature_names[8] = 'LTG' # Log of serum triglycerides level\n",
        "diabetes.feature_names[9] = 'GLU' # Blood sugar level\n",
        "\n",
        "print(f'The various features in the diabetes data base are: {diabetes.feature_names}')\n",
        "print(f'The shape of the input matrix is {dataDiabetes.shape}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's plot the data to see how it looks like.\n",
        "# We are going to plot a matrix of scatter plots to see how the data looks like.\n",
        "# We are going to use the first 5 features in the diabetes dataset.\n",
        "fig, ax = plt.subplots(5, 5, figsize=(12, 12))\n",
        "for i in range(5):\n",
        "    for j in range(5):\n",
        "        if i==j:\n",
        "            ax[i, j].hist(dataDiabetes[:, i])\n",
        "        else:\n",
        "            ax[i, j].scatter(dataDiabetes[:, j], dataDiabetes[:, i])\n",
        "        if j==0:\n",
        "            ax[i, j].set_ylabel(diabetes.feature_names[i])\n",
        "        else:\n",
        "            ax[i, j].set_ylabel('')\n",
        "            ax[i, j].set_yticklabels([])\n",
        "        if i==4:\n",
        "            ax[i, j].set_xlabel(diabetes.feature_names[j])\n",
        "        else:\n",
        "            ax[i, j].set_xlabel('')\n",
        "            ax[i, j].set_xticklabels([])\n",
        "plt.show()\n",
        "# Note that the variables are all scaled somewhat arbitrarilly in this data set.\n",
        "# Some are discrete (e.g., sex), some are continuous (e.g., age).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **1.C. Target Labels ($\\mathbf{Y}$)**\n",
        "\n",
        "![alt text](FiguresD/images.002.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The vector or matrix $\\bar{Y}_i$ is your **target data** or **label data**; this is the end result or classification that you are trying to obtain or predict to describe the sample, event, or experiment. This can too take many shapes, forms, and sizes, and it might not even exist at all.\n",
        "\n",
        "If the above data has a corresponding label (e.g., a known fact such as a statement saying whether the entries in your data truly have diabetes), your dataset is said to be **labeled**, and you can use **Supervised** machine learning techniques. \n",
        "\n",
        "In other machine learning problems, $\\bar{Y}_i$ may not be specified. In this case, when your dataset has an absence of labels, then any approaches are said to be **Unsupervised.**\n",
        "\n",
        "For now, we will focus on supervised learning, and we will assume that $\\bar{Y}_i$ is known.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **1.C.1. Python example of Target Vectors for Diabetes data set.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the target data from the diabetes dataset.\n",
        "# The target data is a quantitative measure of disease progression one year after baseline.\n",
        "targetDiabetesRegression = diabetes.target\n",
        "print(f'The shape of the target data is {targetDiabetesRegression.shape}')\n",
        "print(targetDiabetesRegression[0:5])\n",
        "\n",
        "# Let's also define a binary target variable for classification, where we \n",
        "# only need to classify the disease progression as high or low.\n",
        "targetDiabClass = np.zeros(targetDiabetesRegression.shape)\n",
        "targetDiabClass[targetDiabetesRegression > 140] = 1\n",
        "print(f'The shape of the target data is {targetDiabClass.shape}')\n",
        "print(targetDiabClass[0:5])\n",
        "\n",
        "# Let's add the the target data to see how it looks like.\n",
        "fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
        "ax.hist(targetDiabetesRegression)\n",
        "ax.set_title('Regression target')\n",
        "ax.set_xlabel('Disease progression')\n",
        "ax.set_ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's also replot our features against the target variable to see if we can see any patterns.\n",
        "# For this, we are going to make a 6 x 6 matrix of scatter plots, with the features in the first 5 rows\n",
        "# and the target variable in the last row.  We will also color the points according to the target variable.\n",
        "fig, ax = plt.subplots(6, 6, figsize=(12, 12))\n",
        "data_and_targets = np.hstack((dataDiabetes[:,:5], targetDiabetesRegression.reshape(-1, 1)))\n",
        "for i in range(6):\n",
        "    for j in range(6):\n",
        "        if i==j:\n",
        "            ax[i, j].hist(data_and_targets[:, i])\n",
        "        else:\n",
        "            ax[i, j].scatter(data_and_targets[:, j], data_and_targets[:, i], c=targetDiabetesRegression)\n",
        "        if j==0:\n",
        "            if i==5:\n",
        "                ax[i, j].set_ylabel('Target')\n",
        "            else:\n",
        "                ax[i, j].set_ylabel(diabetes.feature_names[i])\n",
        "        else:\n",
        "            ax[i, j].set_ylabel('')\n",
        "            ax[i, j].set_yticklabels([])\n",
        "            if i==5:\n",
        "                ax[i, j].set_facecolor('#FFDDDD')\n",
        "        if i==5:\n",
        "            ax[i, j].set_facecolor('#FFDDDD')\n",
        "            if j==5:\n",
        "                ax[i, j].set_xlabel('Target')\n",
        "            else:\n",
        "                ax[i, j].set_xlabel(diabetes.feature_names[j])\n",
        "        else:\n",
        "            ax[i, j].set_xlabel('')\n",
        "            ax[i, j].set_xticklabels([])\n",
        "            if j==5:\n",
        "                ax[i, j].set_facecolor('#FFDDDD')\n",
        "\n",
        "# Can you see any patterns in the data?  What do you think would be the best features to use for prediction?\n",
        "# How would you turn code this inot a callable function for later use?\n",
        "# How could you change the color scheme to make to use the binary target variable for a different visualization?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## **1.D. The Model ($\\mathcal{M}$)**\n",
        "![alt text](FiguresD/image5A.png)\n",
        "\n",
        "$\\mathcal{M}$ in our definition is any arbitrary set of mathematical or logical operations that takes $\\bar{X}_i$ as an input and then makes a guess for a corresponding $\\bar{Y}_i$. \n",
        "\n",
        "There are **tons** of different models and algorithms used for fitting data, and we will try a few over the next couple lessons, but this is **extremely** general, and statisticians and computer scientists are developing new models every day!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **1.E. Types of Models: Regression and Classification**\n",
        "The two most common machine learning modeling tasks are **Regression** and **Classification**.  The difference comes from what kinds of targets we are trying to predict. When targets ($\\bar{Y}$) are defined in advance and they take on continuous values (e.g., 2.345), we call this a **Regression** problem. When the targets take on discrete values (e.g., \\{0,1,2\\} or \\{true,false\\}, or \\{cat, horse, dog\\}), we call this a **Classification** problem.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **1.E.1 Regression Analysis**\n",
        "![alt text](FiguresD/images.009.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When the desired output is a continuous valued, then we are training a model to perform a regression. An example problem would be taking a dataset of house features $\\bar{X}$ and their current market prices $\\bar{Y}$ and training a model to produce predicted prices for other houses, $\\tilde{Y} =  \\mathcal{M}(\\tilde{X})$.\n",
        "\n",
        "To evealuate how good is our model, we typically employ a **loss function**, which is simply a function that takes the real targets $\\bar{Y}_i$ and the predicted targets $\\tilde{Y}_i$ and quantifies their difference. \n",
        "\n",
        "An appropriate *loss function* for this situation may be the mean squared error (MSE) for your predicted prices and the true prices (we will why this choice in a little bit when we return to the concept of likelihood functions):\n",
        "\n",
        "\\begin{align}\n",
        "MSE &= \\frac{1}{N}\\sum_{i=0}^{N-1} (\\tilde{Y}_i - \\bar{Y}_i)^2\\\\\n",
        " &= \\frac{1}{N}\\sum_{i=0}^{N-1} (\\mathcal{M}(\\tilde{X}_i) - \\bar{Y}_i)^2\\\\\n",
        "\\end{align}\n",
        "\n",
        "The goal is to find a the best possible $\\mathcal{M}$ to minimize this loss function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **1.E.2. Classification**\n",
        "![alt text](FiguresD/images.010.png)\n",
        "\n",
        "When the desired output is a discrete valued, then we are training a model to perform a classification. An example problem would be taking a dataset of images $\\bar{X}$ and their labels $\\bar{Y}$ and training a model to produce predicted labels for other images, $\\tilde{Y} =  \\mathcal{M}(\\tilde{X})$.\n",
        "\n",
        "$\\bar{Y}$ is our target for our ML model to learn. If our input data label $Y$ is a set of discrete or boolean labels, then we are setting up a model to solve a classification problem. For example, training a classifier to tell if a picture contains a cat or a dog, our dataset would look like $\\bar{X}$ of shape $[N, X,Y, RGB]$ and $Y$ our labels would be shape $[N]$ with labels 0 for a cat or 1 for a dog (or vice versa). Comparing true labels $Y$ with predicted labels $\\bar{Y}$ is our model's loss function:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\bar{L} = F(Y, \\bar{Y})\n",
        "\\end{equation}\n",
        "\n",
        "An appropriate selection for **loss function ** in this classification problem would be binary cross entropy:\n",
        "\n",
        "\\begin{equation}\n",
        "  -\\frac{1}{N} \\sum_{i=1}^N Y_i*\\log(p(Y_i)) + (1-Y_i)*\\log(1-p(Y_i))\n",
        "\\end{equation}\n",
        "\n",
        "This is very closely related to the log likelihood function for a binomial distribution as we will see later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **1.F. What characteristics makes for a good model?**\n",
        "Ideally, we want a model to be generalizable to new data. A good model acts as a map to its real world process. A good map doesn't show you every stone and blade of grass along the path, but instead the overarching **generalized** idea.\n",
        "\n",
        "More generally, a good model in machine learning exhibits several key characteristics that contribute to its effectiveness in solving a given task. \n",
        "* **it should demonstrate high predictive accuracy**, meaning it can accurately generalize patterns from the training data to make accurate predictions on unseen data. \n",
        "* **it should be robust**, performing well across different datasets and under various conditions, indicating its ability to generalize beyond the training data. \n",
        "* **it should be interpretabile**, so stakeholders can understand and trust the model's decisions, especially in sensitive domains. \n",
        "* **it should be scalable** to handle large datasets efficiently and to accommodate potential increases in data volume. \n",
        "* **it should be computationally efficient,** striking a balance between complexity and performance to enable practical deployment in real-world applications.\n",
        "* **it should be adaptable** to new data and evolving requirements, enabling iterative refinement and optimization to maintain its relevance and effectiveness over time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Which of the six models below do you think would be considered a good model?\n",
        "![alt text](FiguresD/images.015.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS6flohrFZGc"
      },
      "source": [
        "-----\n",
        "# **2. Linear Regression**\n",
        "\n",
        "![alt text](FiguresD/images.004.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **2.A. Introduction to Linear Regression**\n",
        "\n",
        "In **Linear Regression** we start with our with $D$ different measured features ($x_1,x_2,\\ldots,x_D$), which _we assume_ are linearly related to the target variable ($y$). To express this mathematically, we define a function which multiplies each feature $x_i$ with a corresponding, unknown weight $w_i$, and add them all together:\n",
        "$$\n",
        "y = \\mathcal{M}(\\mathbf{x}) = w_0 x_0 + w_1 x_1 + ... + w_D x_D.\n",
        "$$\n",
        "We can write this using vector notation as:\n",
        "$$\n",
        "y = \\mathbf{w}^{\\rm T} \\mathbf{x}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G_6-FdtFZGd"
      },
      "source": [
        "## **2.B. Linear equations using matrix multiplication**\n",
        "\n",
        "When we have the multiple data sample sets of $\\mathbf{x}$ and $y$, given by $\\{(\\mathbf{x}_1,y_1),(\\mathbf{x}_2,y_2),\\ldots,(\\mathbf{x}_N,y_N)\\}$ we can construct a matrix, in which each row is defined by the corresponding $\\mathbf{x}_k$.\n",
        "\n",
        "$$\n",
        "\\mathbf{X}=\\begin{bmatrix}\n",
        "\\mathbf{x}_1^T \\\\\n",
        "\\mathbf{x}_2^T \\\\\n",
        "\\vdots \\\\\n",
        "\\mathbf{x}_N^T\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "We want to apply the _same weights_ $\\mathbf{w}$ to each example and return a predicted value, $y_k$.  This can be achieved through a matrix multiplication:\n",
        "$$\n",
        "\\mathbf{y} = \\mathbf{w}^T \\mathbf{X}^T = \\mathbf{X}\\mathbf{w}\n",
        "$$\n",
        "\n",
        "For **real models** and **real data**, this equality is never going to be exact.  Instead, we are going tohave to allow some variations to account for noise or errors in our measurements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g2osIGjFZGf"
      },
      "source": [
        "## **2.C. Loss Function for Linear Regression**\n",
        "\n",
        "So how do we quantify how good is this linear model in comparision to real, noisy data?\n",
        "\n",
        "For this, we want to define what is called the **loss function**, which is a fundamental concept in ML. The loss function is a function which:\n",
        "   * when we take the derivative of it, this can be used to update the weights of the model.\n",
        "   * compares the predicted values of our model to ground truth values (supervised learning) OR takes some metric of the predicted values (unsupervised learning).\n",
        "    \n",
        "Constructing the loss function is a **design choice**. How we choose it can drastically affect the quality of our ML model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLX1pyQlFZGg"
      },
      "source": [
        "## **2.D. Likelihood Approach**\n",
        "\n",
        "Whenever possible, most statisticians prefer to define loss functions in terms of **Likelihood Functions**, since we want our model to be as good as possible in explaining our data in a statistically rigorous sense.\n",
        "\n",
        "Because we are engineers and quantitative biologists, let's assume that the our noisy measurements data are given by our beautiful linear function, plus some real-world noise term $\\epsilon$,\n",
        "$$\n",
        "t_i \\equiv y^{\\rm (measured)}_i = y_i + \\varepsilon = \\mathbf{x}_i\\mathbf{w} + \\varepsilon_i\n",
        "$$\n",
        "where each $\\varepsilon_i$ is an **independent** random number drawn from a Normal distribution with mean 0 and variance $\\sigma^2$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuxERGjaFZGg"
      },
      "source": [
        "Therefore, we can write the likelihood function of the true value given the model prediction for any input $\\mathbf{x}$:\n",
        "\n",
        "$$\n",
        "L(t_i | \\mathbf{x}_i, \\mathbf{w}, \\sigma^2 ) = \\mathcal{N}(t_i|\\mathbf{x}_i\\mathbf{w},\\sigma^2)\n",
        "$$\n",
        "\n",
        "and for all of our *independent* samples as\n",
        "$$\n",
        "L(\\mathbf{t} | \\mathbf{X}, \\mathbf{w}, \\sigma^2 ) = \\prod_{i=1}^N \\mathcal{N}(t_i|\\mathbf{x}_i\\mathbf{w},\\sigma^2)\n",
        "$$\n",
        "\n",
        "**Question:** What assumption did we make here?  When would using the above equation be a bad idea in practice?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoN1TsbOFZGg"
      },
      "source": [
        "### **2.D.1. Transforming to the Log-Likelihood**\n",
        "When we have many data points, the probability to observe them all is going to be very small, even if we have an excelent model. Therefore, it is much more convenient for us to work with log-likelihood functions.\n",
        "$$\n",
        "\\log L(\\mathbf{t}| \\mathbf{X}, \\mathbf{w}, \\sigma^2 ) = \\sum_{i=1}^N \\log \\mathcal{N}(t_i|\\mathbf{x}_i\\mathbf{w},\\sigma^2) =  \\sum_{i=1}^N \\log \\left(\\frac{1}{\\sqrt{2\\pi \\sigma^2)}}\\exp\\left(\\frac{(t_i-\\mathbf{x}_i\\mathbf{w})^2}{2\\sigma^2}\\right) \\right)\n",
        "$$\n",
        "$$\n",
        "\\log L(\\mathbf{t} | \\mathbf{X}, \\mathbf{w}, \\sigma^2 ) = - N \\log \\sigma^2 -  \\frac{N}{2} \\log 2 \\pi - \\frac{1}{2\\sigma^2}  \\sum_{i=1}^N \\big( t_i-\\mathbf{x}_i\\mathbf{w} \\big)^2\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **2.D.2 Differentiating to find the Maximum Likelihood Estimate**\n",
        "Our goal is to find the model weights that **maximize** this log-likelihood function.  And, as we have seen multiple times already, this is done by taking derivatives:\n",
        "$$ \\nabla_{\\mathbf{w}}\\log L(\\mathbf{t} | \\mathbf{X}, \\mathbf{w}, \\sigma^2 ) =    \\nabla_{\\mathbf{w}}\\left(  \\frac{1}{2\\sigma^2}  \\sum_{i=1}^N \\left( t_i-\\mathbf{w}^T\\mathbf{x}_i^T \\right)^2\\right)$$\n",
        "$$=  \\frac{1}{\\sigma^2}  \\sum_{i=1}^N \\left(t_i-\\mathbf{w}^T\\mathbf{x}_i^T\\right)\\mathbf{x}_i.$$\n",
        "\n",
        "Setting this to zero at the maximum likelihood estimate (MLE) and moving the $t_i$ terms to the other  side, we can derive:\n",
        "$$  \\sum_{i=1}^N \\left(\\mathbf{w}_{\\rm MLE}^T\\mathbf{x}_i^T\\right)\\mathbf{x}_i =  \\sum_{i=1}^N  t_i\\mathbf{x}_i$$\n",
        "\n",
        "We can then simplify in matrix notation as:\n",
        "$$\\mathbf{w}_{\\rm MLE}^T\\mathbf{X}^T\\mathbf{X} =   \\mathbf{t}^T\\mathbf{X}^T $$\n",
        "and by taking the transpose of both sides:\n",
        "$$ \\mathbf{X}^T\\mathbf{X} \\mathbf{w}_{\\rm MLE} =   \\mathbf{X} ^T \\mathbf{t}$$\n",
        "which reduces to:\n",
        "$$\\mathbf{w}_{\\rm MLE} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{t} $$\n",
        "\n",
        "In this last expression, the term $(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T$, which is often denoted as $\\mathbf{X}^{+}$ is known as the *Moore-Penrose Pseudo-Inverse* of the matrix $\\mathbf{X}$.  This is a generalization of the common matrix inverse ($\\mathbf{X}^{-1})$, but the Moore-Penrose Pseudo-Inverse works for tall matrices (matrices with more rows than there are columns).  When $\\mathbf{X}$ is invertible, then $\\mathbf{X}^{+}=\\mathbf{X}^{-1}$, but that will rarely be the case in a realistics regression problem where there will be far more rows than columns in the matrix $\\mathbf{X}$.\n",
        "\n",
        "So, after that hard work of maximum likelihood estimation, we arrived at a very intuitive result.  If we have a bunch of observations in the feature matrix $\\mathbf{X}$ we can make the MLE predction for the noisy target vector $\\mathbf{t} = \\mathbf{y}^{\\rm (measured)} = \\mathbf{y}+\\bf{\\varepsilon}$ the equation:\n",
        "$$\\mathbf{t} = \\mathbf{Xw}_{\\rm MLE},$$\n",
        "where the MLE weights are simply:\n",
        "$$\\mathbf{w} = \\mathbf{X}^{+}\\mathbf{t}.$$\n",
        "\n",
        "Now, lets try a few examples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **3. Python Example for Linear Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzpZDxrDFZGd"
      },
      "source": [
        "Now, let's use the [diabetes dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes) to try a linear regression:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **3.A. Load the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GW9ADbWiFZGf"
      },
      "outputs": [],
      "source": [
        "# Load the input data and target data from the diabetes dataset.\n",
        "dataMatrix, targetVector = datasets.load_diabetes(return_X_y=True)\n",
        "\n",
        "# How can we compute the number of features and number of data points?\n",
        "numSamples, numFeatures = dataMatrix.shape\n",
        "\n",
        "print(f'The number of samples is {numSamples}')\n",
        "print(f'The number of features is {numFeatures}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **3.A.1 Split data into training and testing sets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmxBd5dJFZGg"
      },
      "outputs": [],
      "source": [
        "# Let's start by splitting the data into a training and testing set.\n",
        "# We will use 80% of the data for training and 20% for testing.\n",
        "# Partition the data into training and test sets, using random selection.\n",
        "np.random.seed(0)\n",
        "indices = np.random.permutation(numSamples)\n",
        "splitIndex = int(numSamples*0.8)\n",
        "training_inds, test_inds = indices[:splitIndex], indices[splitIndex:]\n",
        "trainingData, testData = dataMatrix[training_inds,:], dataMatrix[test_inds,:]\n",
        "trainingTarget, testTarget = targetVector[training_inds], targetVector[test_inds]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **3.B. Fit the linear regression model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now, let's find the weights according to our derived expression\n",
        "# (X^T X)^-1 X^T y\n",
        "# where X is the training data and y is the training target.\n",
        "weights = np.linalg.inv(trainingData.T @ trainingData) @ trainingData.T @ trainingTarget"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **3.C. Predict the test data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AV7R5hsFFZGg"
      },
      "outputs": [],
      "source": [
        "# Use the weights to predict the target data.\n",
        "predTargetCustom = testData @ weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **3.C.1 Plot the predictions for test data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBilm7qaFZGg",
        "outputId": "b441c2c7-2e26-46b0-806e-d047ee3e5e2b"
      },
      "outputs": [],
      "source": [
        "# Plot the true target data and the predicted target data.\n",
        "f,ax = plt.subplots(figsize=(4,3))\n",
        "ax.scatter(testTarget, predTargetCustom)\n",
        "ax.set_xlabel('true value'); ax.set_ylabel('predicted value')\n",
        "ax.set_title('True vs predicted target data')\n",
        "#Put a line through the origin with slope 1\n",
        "ax.plot(ax.get_xlim(), ax.get_xlim(), 'k--')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **3.C.2 Evaluate the model using the R^2 score**\n",
        "\n",
        "The R^2 score, also known as the **coefficient of determination**, is a measure of how well the observed outcomes are replicated by the model.  It is a number less than or equal to 1, with 1 indicating perfect fit. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's compute the R^2 score to quantify the fit.\n",
        "r2_LinReg = r2_score(testTarget, predTargetCustom)\n",
        "print(f'The R^2 score is {r2_LinReg}')\n",
        "# Yuck, negative values can indicate that the model is worse than just using the mean of the target data!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **3.D. Account for model bias and overfitting**\n",
        "\n",
        "### **3.D.1. Add constant term to account for bias**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ba_P3tqFFZGh"
      },
      "outputs": [],
      "source": [
        "# Let's see if we can do better by adding a bias term to the model.\n",
        "# We can add a bias term by adding a column of ones to the input data matrix.\n",
        "# This is equivalent to adding a constant to the input data.\n",
        "X_biasTrain = np.hstack((trainingData, np.ones((trainingData.shape[0], 1))))\n",
        "X_biasTest = np.hstack((testData, np.ones((testData.shape[0], 1))))\n",
        "\n",
        "# find the weights according to our derived expression\n",
        "w_bias = np.linalg.inv(X_biasTrain.T @ X_biasTrain) @ X_biasTrain.T @ trainingTarget\n",
        "y_biasPredict = X_biasTest @ w_bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "w_bias[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fOCyhPEFZGh",
        "outputId": "8aec603d-fb0f-4259-fa09-58015830b8ae"
      },
      "outputs": [],
      "source": [
        "# Now, let's plot the true target data and the predicted target data.\n",
        "f,ax = plt.subplots(figsize=(4,3))\n",
        "ax.scatter(testTarget, y_biasPredict)\n",
        "ax.set_xlabel('true value'); ax.set_ylabel('predicted value (with bias)')\n",
        "ax.set_title('True vs predicted target data')\n",
        "#Put a line through the origin with slope 1\n",
        "ax.plot(ax.get_xlim(), ax.get_xlim(), 'k--')\n",
        "plt.show()\n",
        "\n",
        "# Now, let's compute the R^2 score to quantify the fit.\n",
        "r2_LinRegBias = r2_score(testTarget, y_biasPredict)\n",
        "print(f'The R^2 score is {r2_LinRegBias}')\n",
        "\n",
        "# That's better!  The R^2 score is now positive, indicating that the \n",
        "# model is better than just using the mean of the target data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfdYF400FZGj"
      },
      "source": [
        "### **3.D.2. Use Regularization to prevent overfitting**\n",
        "\n",
        "![alt text](FiguresD/images.007.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sometimes, we don't want the weights of a model to be unrealistically large, as this can lead to unnecessary overfitting. To avoid this, we can assume a **prior** on the expected values of the parameters, which will penalize unexpected large values of the weights (we will learn more about priors in Bayesian estimation later in the course). By using an independent Gaussian prior with mean zero and variance $\\sqrt{2/\\lambda}$ on every weight, we are saying that we expect each parameter $w_id$ to be zero unless that parameter is trully needed to match the data. \n",
        "\n",
        "With this addition, the loss function becomes:\n",
        "$$\n",
        "\\mathcal{L}_{\\rm ridge}(\\mathbf{X},\\mathbf{w},\\lambda) = C + \\frac{1}{N} \\sum_{i=1}^N \\big( t_i-\\mathbf{x}_i\\mathbf{w} \\big)^2 +\\lambda \\sum_{d=1}^{D+1}  w_d ^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\rm ridge}(\\mathbf{X},\\mathbf{w},\\lambda) = C + ( \\mathbf{t} - \\mathbf{X} \\mathbf{w} )^T( \\mathbf{t} - \\mathbf{X} \\mathbf{w} ) +  \\lambda \\mathbf{w}^T \\mathbf{w}\n",
        "$$\n",
        "\n",
        "where $C$ is some constant that does not depend on $\\mathbf{w}$.\n",
        "\n",
        "Like before, we can differentiate with respect to $\\mathbf{w}$ to find the maximum, and we will find the answer:\n",
        "$$\n",
        "\\mathbf{w}_{\\rm Ridge} = (\\mathbf{X}^T \\cdot \\mathbf{X} + \\lambda^2 I)^{-1}\\mathbf{X}^T \\cdot \\mathbf{t}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### **3.D.3. Hyperparameter Tuning**\n",
        "Before we go on to an example, please note that we have just introduced a new parameter $\\lambda$, which is known as the *regularization parameter*.  This is our first example of model *hyperparameter*.\n",
        "\n",
        "In machine learning, hyperparameters are parameters that are set prior information into the training process or control the behavior of the learning algorithm itself. They are not learned from the training data but are instead specified by the practitioner based on their prior knowledge, experience, or through techniques like grid search or random search.\n",
        "\n",
        "Hyperparameters affect the learning process indirectly by influencing the model's capacity, complexity, and regularization. Examples of hyperparameters include the regularization parameter in models like ridge regression or support vector machines (as shown here), but also include the learning rate in gradient descent, the depth of a decision tree, the number of hidden layers in a neural network, and so on.\n",
        "\n",
        "Tuning hyperparameters effectively is crucial for achieving optimal model performance, as different values can lead to vastly different results in terms of predictive accuracy, generalization, and computational efficiency.  This tuning is often done using cross-validation, which is why we split up our data in the beginning of this exercise.  We will see more on this shortly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SR-SzaLKFZGj"
      },
      "outputs": [],
      "source": [
        "# Find the weights (w) according to our derived expression using the ridge regression\n",
        "lam = np.sqrt(0.1)  # Set our hyper parameter.  This is a guess for now.\n",
        "\n",
        "# Compute the weights using the ridge regression formula.\n",
        "w_ridge = np.linalg.inv(X_biasTrain.T @ X_biasTrain + (lam**2)*np.eye(X_biasTrain.shape[1])) @ X_biasTrain.T @ trainingTarget\n",
        "\n",
        "# Use the weights to predict the target data.\n",
        "y_ridgeTest = X_biasTest @ w_ridge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwnlIc1NFZGj",
        "outputId": "c6e8045d-fa68-4c3a-e1c9-c5cb0181df0c"
      },
      "outputs": [],
      "source": [
        "# Now, let's plot the true target data and the predicted target data.\n",
        "f,ax = plt.subplots(figsize=(4,3))\n",
        "ax.scatter(testTarget, y_biasPredict, alpha=0.5, label='linear regression')\n",
        "ax.scatter(testTarget, y_ridgeTest, alpha=0.5, label='ridge regression')\n",
        "ax.set_xlabel('true value'); ax.set_ylabel('predicted value')\n",
        "ax.legend()\n",
        "ax.set_title('True vs predicted target data')\n",
        "\n",
        "#Put a line through the origin with slope 1\n",
        "ax.plot(ax.get_xlim(), ax.get_xlim(), 'k--')\n",
        "plt.show()\n",
        "\n",
        "# Now, let's compute the R^2 score to quantify the fit.\n",
        "r2_RidgeTuned = r2_score(testTarget, y_ridgeTest)\n",
        "print(f'The R^2 score is {r2_RidgeTuned}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dQ1jrq1FZGj"
      },
      "outputs": [],
      "source": [
        "# Now let's try to find the best value of lambda.  This process is called hyperparameter tuning.\n",
        "\n",
        "# Let's look at weights at different levels of lamba, and get the r2 score for each.\n",
        "lambs = np.logspace(-5,0.2,50)\n",
        "all_train_scores = []\n",
        "all_val_scores = []\n",
        "\n",
        "# Split our training data into a training and validation set.\n",
        "np.random.seed(0)\n",
        "indices = np.random.permutation(trainingData.shape[0])\n",
        "splitIndex = int(trainingData.shape[0]*0.8)\n",
        "training_inds, val_inds = indices[:splitIndex], indices[splitIndex:]\n",
        "trainDat, valDat = trainingData[training_inds,:], trainingData[val_inds,:]\n",
        "trainTarg, valTarg = trainingTarget[training_inds], trainingTarget[val_inds]\n",
        "XTrain = np.hstack((trainDat, np.ones((trainDat.shape[0], 1))))\n",
        "XVal = np.hstack((valDat, np.ones((valDat.shape[0], 1))))\n",
        "\n",
        "for lamb in lambs:\n",
        "    # Compute the weights using the ridge regression formula.\n",
        "    w_ridge = np.linalg.inv(XTrain.T @ XTrain + lamb*np.eye(XTrain.shape[1])) @ XTrain.T @ trainTarg\n",
        "    # Use the weights to predict the target data in training and testing sets.\n",
        "    y_ridge_train = XTrain @ w_ridge\n",
        "    y_ridge_val = XVal @ w_ridge\n",
        "    # Now, let's compute the R^2 score to quantify the fit and store it.\n",
        "    all_train_scores.append(r2_score(trainTarg, y_ridge_train))\n",
        "    all_val_scores.append(r2_score(valTarg, y_ridge_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqKAjlo7FZGk",
        "outputId": "06cfc4df-138f-47fa-9fb5-7a3ba7483734"
      },
      "outputs": [],
      "source": [
        "# Let's look at train and validation scores, and try to pick the best lambda.\n",
        "f,ax = plt.subplots()\n",
        "ax.plot(lambs, all_train_scores,label='train')\n",
        "ax.plot(lambs, all_val_scores, label='validation')\n",
        "ax.legend()\n",
        "ax.set_xlabel('lambda')\n",
        "ax.set_ylabel('r2_score')\n",
        "ax.set_xscale('log')\n",
        "\n",
        "# find the best lambda\n",
        "best_lambda = lambs[np.argmax(all_val_scores)]\n",
        "print(f'The best lambda is {best_lambda}')\n",
        "\n",
        "# Add star to the plot denoting the best lambda\n",
        "ax.plot(best_lambda, np.max(all_val_scores), 'r*', markersize=10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find the weights according to our derived expression using the ridge regression\n",
        "w_ridge = np.linalg.inv(X_biasTrain.T @ X_biasTrain + best_lambda*np.eye(X_biasTrain.shape[1])) @ X_biasTrain.T @ trainingTarget\n",
        "y_ridgeOpt = X_biasTest @ w_ridge\n",
        "\n",
        "# Now, let's plot the true target data and the predicted target data for the Testing data.\n",
        "f,ax = plt.subplots(figsize=(4,3))\n",
        "ax.scatter(testTarget, y_biasPredict, alpha=0.5, label='linear regression')\n",
        "ax.scatter(testTarget, y_ridgeOpt, alpha=0.5, label='ridge regression (optimal lambda)')\n",
        "ax.set_xlabel('true value'); ax.set_ylabel('predicted value')\n",
        "ax.legend()\n",
        "\n",
        "ax.set_title('True vs predicted target data')\n",
        "\n",
        "#Put a line through the origin with slope 1\n",
        "ax.plot(ax.get_xlim(), ax.get_xlim(), 'k--')\n",
        "plt.show()\n",
        "\n",
        "# Now, let's compute the R^2 score to quantify the fit.\n",
        "r2_LinRegBias = r2_score(testTarget, y_ridgeOpt)\n",
        "print(f'The R^2 score with optimized ridge regression is {r2_LinRegBias}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **3.D.4. Other regularization methods**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Other regularization methods\n",
        "# Lasso regression (Least Absolute Shrinkage and Selection Operator or L1 regularization)\n",
        "from sklearn.linear_model import Lasso\n",
        "lasso = Lasso(alpha=0.1)\n",
        "lasso.fit(X_biasTrain, trainingTarget)\n",
        "y_lasso = lasso.predict(X_biasTest)\n",
        "\n",
        "# Now, let's plot the true target data and the predicted target data.\n",
        "f,ax = plt.subplots(figsize=(4,3))\n",
        "ax.scatter(testTarget, y_lasso)\n",
        "ax.set_xlabel('true value'); ax.set_ylabel('predicted value')\n",
        "ax.set_title('True vs predicted target data')\n",
        "#Put a line through the origin with slope 1\n",
        "ax.plot(ax.get_xlim(), ax.get_xlim(), 'k--')\n",
        "plt.show()\n",
        "\n",
        "# Now, let's compute the R^2 score to quantify the fit.\n",
        "r2_Lasso = r2_score(testTarget, y_lasso)\n",
        "print(f'The R^2 score with lasso regression is {r2_Lasso}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's look at the residuals for each of the methods\n",
        "# Linear regression\n",
        "residuals_linear = testTarget - y_biasPredict\n",
        "# Ridge regression\n",
        "residuals_ridge = testTarget - y_ridgeOpt\n",
        "# Lasso regression\n",
        "residuals_lasso = testTarget - y_lasso\n",
        "\n",
        "# Now, let's plot the residuals for each method\n",
        "f,ax = plt.subplots(figsize=(4,3))\n",
        "ax.scatter(testTarget, residuals_linear, alpha=0.5, label='linear regression')\n",
        "ax.scatter(testTarget, residuals_ridge, alpha=0.5, label='ridge regression')\n",
        "ax.scatter(testTarget, residuals_lasso, alpha=0.5, label='lasso regression')\n",
        "ax.set_xlabel('true value'); ax.set_ylabel('residuals')\n",
        "ax.legend()\n",
        "ax.set_title('Residuals for different regression methods')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.E. Linear Regression for Polynomial Features\n",
        "\n",
        "The features used in the linear regression model above were all linear.  But what if we have a non-linear relationship between the features and the target variable?  One way to address this is to use polynomial features.  This is a simple way to add complexity to the model, and can be done by adding higher order terms of the features to the model.  For example, if we have a single feature $x$, we can add a quadratic term $x^2$ to the model, or a cubic term $x^3$, etc.  This can be done for multiple features as well. Let's see how this works in practice by doing a linear regression on a dataset with polynomial features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Linear regression with polynomial features up to order 2\n",
        "lambda_ = 0.001\n",
        "\n",
        "X_biasTrainExtended = X_biasTrain\n",
        "for i in range(numFeatures):\n",
        "    for j in range(i, numFeatures):\n",
        "        X_biasTrainExtended = np.hstack((X_biasTrainExtended, (X_biasTrain[:,i]*X_biasTrain[:,j]).reshape(-1,1)))\n",
        "X_biasTestExtended = X_biasTest\n",
        "for i in range(numFeatures):\n",
        "    for j in range(i, numFeatures):\n",
        "        X_biasTestExtended = np.hstack((X_biasTestExtended, (X_biasTest[:,i]*X_biasTest[:,j]).reshape(-1,1)))\n",
        "\n",
        "# Find the weights using Ridge regression\n",
        "w_ridge = np.linalg.inv(X_biasTrainExtended.T @ X_biasTrainExtended + lambda_*np.eye(X_biasTrainExtended.shape[1])) @ X_biasTrainExtended.T @ trainingTarget\n",
        "y_ridgeOpt = X_biasTestExtended @ w_ridge\n",
        "\n",
        "# Now, let's plot the true target data and the predicted target data for the Testing data.\n",
        "f,ax = plt.subplots(figsize=(4,3))\n",
        "ax.scatter(testTarget, y_ridgeOpt)\n",
        "ax.set_xlabel('true value'); ax.set_ylabel('predicted value')\n",
        "ax.set_title('True vs predicted target data')\n",
        "#Put a line through the origin with slope 1\n",
        "ax.plot(ax.get_xlim(), ax.get_xlim(), 'k--')\n",
        "plt.show()\n",
        "\n",
        "# Now, let's compute the R^2 score to quantify the fit.\n",
        "r2_LinRegPoly2 = r2_score(testTarget, y_ridgeOpt)\n",
        "print(f'The R^2 score with optimized ridge regression (Poly2) is {r2_LinRegPoly2}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Linear Regression with Polynomial Features up to Order 3\n",
        "lambda_ = 0.001\n",
        "\n",
        "X_biasTrainExtended = X_biasTrain\n",
        "for i in range(numFeatures):\n",
        "    for j in range(i, numFeatures):\n",
        "        X_biasTrainExtended = np.hstack((X_biasTrainExtended, (X_biasTrain[:,i]*X_biasTrain[:,j]).reshape(-1,1)))\n",
        "for i in range(numFeatures):\n",
        "    for j in range(i, numFeatures):\n",
        "        for k in range(j, numFeatures):\n",
        "            X_biasTrainExtended = np.hstack((X_biasTrainExtended, (X_biasTrain[:,i]*X_biasTrain[:,j]*X_biasTrain[:,k]).reshape(-1,1)))\n",
        "X_biasTestExtended = X_biasTest\n",
        "for i in range(numFeatures):\n",
        "    for j in range(i, numFeatures):\n",
        "        X_biasTestExtended = np.hstack((X_biasTestExtended, (X_biasTest[:,i]*X_biasTest[:,j]).reshape(-1,1)))\n",
        "for i in range(numFeatures):\n",
        "    for j in range(i, numFeatures):\n",
        "        for k in range(j, numFeatures):\n",
        "            X_biasTestExtended = np.hstack((X_biasTestExtended, (X_biasTest[:,i]*X_biasTest[:,j]*X_biasTest[:,k]).reshape(-1,1)))\n",
        "\n",
        "# Find the weights using Ridge regression\n",
        "w_ridge = np.linalg.inv(X_biasTrainExtended.T @ X_biasTrainExtended + lambda_*np.eye(X_biasTrainExtended.shape[1])) @ X_biasTrainExtended.T @ trainingTarget\n",
        "y_ridgeOpt = X_biasTestExtended @ w_ridge\n",
        "\n",
        "# Now, let's plot the true target data and the predicted target data for the Testing data.\n",
        "f,ax = plt.subplots(figsize=(4,3))\n",
        "ax.scatter(testTarget, y_ridgeOpt)\n",
        "ax.set_xlabel('true value'); ax.set_ylabel('predicted value')\n",
        "ax.set_title('True vs predicted target data')\n",
        "#Put a line through the origin with slope 1\n",
        "ax.plot(ax.get_xlim(), ax.get_xlim(), 'k--')\n",
        "plt.show()\n",
        "\n",
        "# Now, let's compute the R^2 score to quantify the fit.\n",
        "r2_LinRegPoly3 = r2_score(testTarget, y_ridgeOpt)\n",
        "print(f'The R^2 score with optimized ridge regression (Poly3) is {r2_LinRegPoly3}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **4. Modern Machine Learning Tools for Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_z_zVCoNGRF"
      },
      "source": [
        "Now that we have seen the basics of regression and know what are models and how to use them, let's play around with some more modern tools.  We don;t have time to cover the theory in detail, but many of the concepts are identical (e.g., features, targets, models, loss functions, regularization, hyperparameters, training/validation/testing). \n",
        "\n",
        "Python is one of the most popular platforms for machine learning, and the three main libraries used for machine learning are:\n",
        "\n",
        "* [Tensorflow](https://www.tensorflow.org/)\n",
        "* [PyTorch](https://pytorch.org/)\n",
        "* [Sci-kit Learn](https://scikit-learn.org/stable/)\n",
        "\n",
        "Tensorflow and Pytorch are the main packages for creating custom neural networks while Sci-kit learn focuses on providing common ML methods. Pytorch and Tensorflow (as of 2022) account for roughly 50% and 25% of new machine learning repositories in GitHub, respectively. Much digital ink has been spilled extolling the pros and cons of each framework; however, my personal best advice is to use whichever package has previous models for your particular problem uncovered by a literature search.\n",
        "\n",
        "![tf_vs_pytorch](https://raw.githubusercontent.com/MunskyGroup/uqbio2021/main/module_2/figures/pytorch_vs_tensorflow.png)\n",
        "\n",
        "\n",
        "For this notebook, we will be using pyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **4.A. Neural Networks**\n",
        "The technique of Neural Networks in Machine Learning can be thought of as as regression on steroids.  Instead of just one round of weights between the features and the targets, we introduce multiple sequential layers, where the outputs of each form the inputs to the next layer.  In addition, the functions are not just simple linear functions, but are replaced with other differentiable functions $f(x,\\mathbf{w})$ at each stage.  \n",
        "\n",
        "![alt text](FiguresD/images.005.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **4.A.1. Python example of a Neural Network in PyTorch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reshape the target data to be a n_samples x 1 matrix. This is required by PyTorch.\n",
        "trainingTarget = trainingTarget.reshape(-1, 1)\n",
        "testTarget = testTarget.reshape(-1, 1)\n",
        "\n",
        "# Convert the data to PyTorch tensors\n",
        "XTrain = torch.from_numpy(X_biasTrain).float()\n",
        "yTrain = torch.from_numpy(trainingTarget).float()\n",
        "XTest = torch.from_numpy(X_biasTest).float()\n",
        "yTest = torch.from_numpy(testTarget).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple neural network model. This is a linear model with a single output. \n",
        "# It is equivalent to the linear regression model we used above.\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc = nn.Linear(X_biasTrain.shape[1], 1)\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "    \n",
        "# Create the model, loss function, and optimizer\n",
        "model = SimpleNN()\n",
        "# Here we will use the MSE loss function (same as above) and the SGD optimizer.\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Create a DataLoader for the training data\n",
        "trainData = TensorDataset(XTrain, yTrain)\n",
        "trainLoader = DataLoader(trainData, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model, using the training data\n",
        "epochs = 3000\n",
        "losses = []\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainLoader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    losses.append(running_loss)\n",
        "    # Print the loss every 100 epochs, to see how the training is going\n",
        "    if epoch % 100 == 99:\n",
        "        print(f'Epoch {epoch+1}, loss: {running_loss:.2f}')\n",
        "\n",
        "# As you run this, you should see the loss decrease over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the loss over time, to see if the model is converging.\n",
        "f,ax = plt.subplots(figsize=(3,3))\n",
        "ax.plot(losses)\n",
        "ax.set_xlabel('epoch')\n",
        "ax.set_ylabel('loss')\n",
        "ax.set_title('Loss over time')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the model to predict the target data in the testing set\n",
        "y_nn = model(XTest).detach().numpy()\n",
        "\n",
        "# Now, let's plot the true target data and the predicted target data.\n",
        "f,ax = plt.subplots(figsize=(3,3))\n",
        "ax.scatter(testTarget, y_nn)\n",
        "ax.set_xlabel('true value'); ax.set_ylabel('predicted value')\n",
        "ax.set_title('True vs predicted target data')\n",
        "#Put a line through the origin with slope 1\n",
        "ax.plot(ax.get_xlim(), ax.get_xlim(), 'k--')\n",
        "plt.show()\n",
        "\n",
        "# Now, let's compute the R^2 score to quantify the fit.\n",
        "r2_NN = r2_score(testTarget, y_nn)\n",
        "print(f'The R^2 score with neural network is {r2_NN}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now, let's try a more complex model, with a couple of hidden layers.\n",
        "input_size = X_biasTrain.shape[1]\n",
        "hidden_size = [10, 5]\n",
        "output_size = 1\n",
        "\n",
        "class ComplexNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ComplexNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size[0])\n",
        "        self.fc2 = nn.Linear(hidden_size[0], hidden_size[1])\n",
        "        self.fc3 = nn.Linear(hidden_size[1], output_size)\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "    \n",
        "# This model consists of three fully connected layers, with 20, 10, and 1 neurons, respectively.\n",
        "# The first two layers use the ReLU activation function, while the last layer does not use any activation function.\n",
        "# See if you can figure out where these numbers are specified in the code above.\n",
        "    \n",
        "# Create the model, loss function, and optimizer\n",
        "model = ComplexNN()\n",
        "criterion = nn.MSELoss()\n",
        "# Choose the optimizer and learning rate.  \n",
        "# Here we will use the stochastic gradient descent (SGD) optimizer. This takes a learning rate as an argument, which controls \n",
        "# how quickly the model learns. A larger learning rate will make the model learn faster, but it may also make the model\n",
        "# less stable. A smaller learning rate will make the model learn more slowly, but it may also make the model more stable.\n",
        "# The learning rate is a hyperparameter that you can tune to improve the performance of your model.\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.00001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "# Train the model, using the training data\n",
        "epochs = 10000\n",
        "losses = []\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainLoader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    losses.append(running_loss)\n",
        "    # Print the loss every 100 epochs, to see how the training is going.\n",
        "    # Delete the output of the print statement each time you run this, to keep the output clean.\n",
        "    if epoch % 100 == 0:\n",
        "        # clear_output(wait=True)\n",
        "        print(f'Epoch {epoch+1}, loss: {running_loss:.2f}\\r\\r')\n",
        "        \n",
        "# As you run this, you should see the loss decrease over time.\n",
        "# Run it multiple times to see how the loss changes.\n",
        "# Try different learning rates and numbers of epochs to see how the loss changes.\n",
        "# Try different numbers of hidden layers and neurons to see how the loss changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the loss over time, to see if the model is converging.\n",
        "f,ax = plt.subplots(figsize=(3,3))\n",
        "ax.plot(losses)\n",
        "ax.set_xlabel('epoch')\n",
        "ax.set_ylabel('loss')\n",
        "ax.set_title('Loss over time')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the model to predict the target data in the testing set\n",
        "y_nn = model(XTest).detach().numpy()\n",
        "\n",
        "# Now, let's plot the true target data and the predicted target data.\n",
        "f,ax = plt.subplots(figsize=(3,3))\n",
        "ax.scatter(testTarget, y_nn)\n",
        "ax.set_xlabel('true value'); ax.set_ylabel('predicted value')\n",
        "ax.set_title('True vs predicted target data')\n",
        "#Put a line through the origin with slope 1\n",
        "ax.plot(ax.get_xlim(), ax.get_xlim(), 'k--')\n",
        "plt.show()\n",
        "\n",
        "# Now, let's compute the R^2 score to quantify the fit.\n",
        "r2_NNComplex = r2_score(testTarget, y_nn)\n",
        "print(f'The R^2 score with larger neural network is {r2_NNComplex}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **4.B. Decision Trees and Random Forests**\n",
        "\n",
        "**Decision Trees** are a type of model that makes decisions based on a series of if-then-else statements. For example, a decision tree might ask if a patient has a fever, and if they do, it might ask if they have a cough, and so on.  The final decision is made based on the answers to these questions.\n",
        "\n",
        "**Random Forests** are a type of ensemble learning method, where multiple decision trees are trained on different subsets of the data and then combined to make a final prediction. This can help to reduce overfitting and improve the accuracy of the model.\n",
        "\n",
        "## **4.B.1. Python example of a Random Forest in Sci-kit Learn**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Regression using Random Forest in scikit-learn\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Create the model\n",
        "rf = RandomForestRegressor(n_estimators=100)\n",
        "\n",
        "# Train the model\n",
        "rf.fit(X_biasTrain, trainingTarget)\n",
        "\n",
        "# Use the model to predict the target data in the testing set\n",
        "y_rf = rf.predict(X_biasTest)\n",
        "\n",
        "# Now, let's plot the true target data and the predicted target data.\n",
        "f,ax = plt.subplots(figsize=(3,3))\n",
        "ax.scatter(testTarget, y_rf)\n",
        "ax.set_xlabel('true value'); ax.set_ylabel('predicted value')\n",
        "ax.set_title('True vs predicted target data')\n",
        "#Put a line through the origin with slope 1\n",
        "ax.plot(ax.get_xlim(), ax.get_xlim(), 'k--')\n",
        "plt.show()\n",
        "\n",
        "# Now, let's compute the R^2 score to quantify the fit.\n",
        "r2_RF = r2_score(testTarget, y_rf)\n",
        "print(f'The R^2 score with random forest is {r2_RF}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.C. Bayesian Regression\n",
        "\n",
        "Bayesian regression is a type of regression analysis that uses Bayesian statistics to estimate the parameters of a regression model. In Bayesian regression, we start with a prior distribution for the parameters of the model, and then update this distribution based on the observed data to get a posterior distribution for the parameters. This allows us to quantify the uncertainty in our estimates and make probabilistic predictions.\n",
        "\n",
        "## **4.C.1. Python example of Bayesian Regression in PyMC3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bayesian regression\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "# Create the model\n",
        "bayesian = BayesianRidge()\n",
        "\n",
        "# Train the model\n",
        "bayesian.fit(X_biasTrain, trainingTarget)\n",
        "\n",
        "# Use the model to predict the target data in the testing set\n",
        "y_bayesian = bayesian.predict(X_biasTest)\n",
        "\n",
        "# Now, let's plot the true target data and the predicted target data.\n",
        "f,ax = plt.subplots(figsize=(3,3))\n",
        "ax.scatter(testTarget, y_bayesian)\n",
        "ax.set_xlabel('true value'); ax.set_ylabel('predicted value')\n",
        "ax.set_title('True vs predicted target data')\n",
        "#Put a line through the origin with slope 1\n",
        "ax.plot(ax.get_xlim(), ax.get_xlim(), 'k--')\n",
        "plt.show()\n",
        "\n",
        "# Now, let's compute the R^2 score to quantify the fit.\n",
        "r2_Bayesian = r2_score(testTarget, y_bayesian)\n",
        "print(f'The R^2 score with Bayesian regression is {r2_Bayesian}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bayesian regression with polynomial features up to order 3\n",
        "# Create the model\n",
        "bayesian = BayesianRidge()\n",
        "\n",
        "# Train the model\n",
        "bayesian.fit(X_biasTrainExtended, trainingTarget)\n",
        "\n",
        "# Use the model to predict the target data in the testing set\n",
        "y_bayesian = bayesian.predict(X_biasTestExtended)\n",
        "\n",
        "# Now, let's plot the true target data and the predicted target data.\n",
        "f,ax = plt.subplots(figsize=(3,3))\n",
        "ax.scatter(testTarget, y_bayesian)\n",
        "ax.set_xlabel('true value'); ax.set_ylabel('predicted value')\n",
        "ax.set_title('True vs predicted target data')\n",
        "#Put a line through the origin with slope 1\n",
        "ax.plot(ax.get_xlim(), ax.get_xlim(), 'k--')\n",
        "plt.show()\n",
        "\n",
        "# Now, let's compute the R^2 score to quantify the fit.\n",
        "r2_BayesianPoly3 = r2_score(testTarget, y_bayesian)\n",
        "\n",
        "print(f'The R^2 score with Bayesian regression (Poly3) is {r2_BayesianPoly3}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **5. Summary**\n",
        "\n",
        "In this notebook, we have covered the basics of regression, including linear regression, regularization, and modern machine learning tools. We have seen how to fit a linear regression model to data, how to evaluate the model using the R^2 score, and how to account for model bias and overfitting using regularization. We have also seen how to use modern machine learning tools like neural networks, decision trees, random forests, and Bayesian regression to solve regression problems.\n",
        "\n",
        "Let's summarize the key points we have covered:\n",
        "\n",
        "* **Regression** is a type of machine learning that is used to predict a continuous target variable based on one or more input features.\n",
        "* **Linear regression** is a simple regression model that assumes a linear relationship between the input features and the target variable.\n",
        "* **Linear regression with polynomial features** is a way to add complexity to the model by including higher order terms of the features.\n",
        "* **Regularization** is a technique used to prevent overfitting in machine learning models by adding a penalty term to the loss function.\n",
        "* **Neural networks** are a type of machine learning model that uses multiple layers of interconnected nodes to learn complex patterns in the data.\n",
        "* **Decision trees** are a type of model that makes decisions based on a series of if-then-else statements.\n",
        "* **Random forests** are an ensemble learning method that combines multiple decision trees to make a final prediction.\n",
        "\n",
        "Also, let's summarize the performance of the models we have used:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make a table of the model performance\n",
        "import pandas as pd\n",
        "data = {'Model': ['Linear Regression', 'Ridge Regression', 'Lasso Regression', 'Linear Regression (Poly2)', 'Linear Regression (Poly3)', 'Neural Network', 'Neural Network (Complex)', 'Random Forest', 'Bayesian Regression', 'Bayesian Regression (Poly3)'],\n",
        "        'R^2 Score': [r2_LinRegBias, r2_RidgeTuned, r2_Lasso, r2_LinRegPoly2, r2_LinRegPoly3, r2_NN, r2_NNComplex, r2_RF, r2_Bayesian, r2_BayesianPoly3]}\n",
        "df = pd.DataFrame(data)\n",
        "print(df)"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "068063431c65405a83444aa2524f5343": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2257a2f54cec4c70a3781c6ca4c6fca5",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fbc2bc5c7b7b40aab96a2c3d6c78e9ea",
            "value": 500
          }
        },
        "18ba693441b04396aebd7795c5d4eb58": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c7d17eb53574c4e9ec9ecc7f033c6b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f131c198d5047a1967487d2f29fa297": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e607e2a6258c4060b779653903fc0359",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2db5fcdaf0c648978dbdc32d215bbc5b",
            "value": 500
          }
        },
        "2257a2f54cec4c70a3781c6ca4c6fca5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22643903e45c4f2d88d63615c8e827c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2db5fcdaf0c648978dbdc32d215bbc5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c4539d8e1c846f882a53ef00322736e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22643903e45c4f2d88d63615c8e827c8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e778c187292141ecba2ccaa513c49499",
            "value": " 500/500 [02:23&lt;00:00,  2.97it/s]"
          }
        },
        "46beb58c035f4e32b8eb2e57b4a7ac3b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63cd95068fd644068a353820af99a01a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8b860974fcc34eabbba0a32a9afd83fa",
              "IPY_MODEL_1f131c198d5047a1967487d2f29fa297",
              "IPY_MODEL_ffb635e851d741a082f5a6bcb419c442"
            ],
            "layout": "IPY_MODEL_a5687b512e8844d4a65a0caa79e179e3"
          }
        },
        "6552e6fab600405fac76432672d85b83": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67bb0ba205d349fd812d28d67cfa3f24": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d43e79082b0d4c849fc950d78c352793",
              "IPY_MODEL_068063431c65405a83444aa2524f5343",
              "IPY_MODEL_3c4539d8e1c846f882a53ef00322736e"
            ],
            "layout": "IPY_MODEL_46beb58c035f4e32b8eb2e57b4a7ac3b"
          }
        },
        "75417cc148594f188a108d6c1e622772": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b860974fcc34eabbba0a32a9afd83fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6552e6fab600405fac76432672d85b83",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1c7d17eb53574c4e9ec9ecc7f033c6b0",
            "value": "100%"
          }
        },
        "a5687b512e8844d4a65a0caa79e179e3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d43e79082b0d4c849fc950d78c352793": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18ba693441b04396aebd7795c5d4eb58",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_75417cc148594f188a108d6c1e622772",
            "value": "100%"
          }
        },
        "e607e2a6258c4060b779653903fc0359": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e729726846f745f78ebb4a3666eecef9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e778c187292141ecba2ccaa513c49499": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eecfc5cffea24fd38f36bf5ff8d853cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fbc2bc5c7b7b40aab96a2c3d6c78e9ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ffb635e851d741a082f5a6bcb419c442": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e729726846f745f78ebb4a3666eecef9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_eecfc5cffea24fd38f36bf5ff8d853cf",
            "value": " 500/500 [01:03&lt;00:00,  7.47it/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
